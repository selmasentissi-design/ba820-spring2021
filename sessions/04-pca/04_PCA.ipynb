{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-PCA",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtJUaqXgQEMg"
      },
      "source": [
        "##############################################################################\n",
        "## Dimension Reduction 1: Principal Components Analysis\n",
        "## Learning goals:\n",
        "## - application of PCA in python via sklearn\n",
        "## - data considerations and assessment of fit\n",
        "## - hands on data challenge = Put all of your skills from all courses together!\n",
        "## - setup non-linear discussion for next session\n",
        "##\n",
        "##############################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9__nyU-9WRAY"
      },
      "source": [
        "# installs\n",
        "\n",
        "# notebook/colab\n",
        "# ! pip install scikit-plot\n",
        "\n",
        "# local/server\n",
        "# pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U1hIFseWdZ7"
      },
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# what we need for today\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics \n",
        "\n",
        "import scikitplot as skplt\n",
        "\n",
        "# color maps\n",
        "from matplotlib import cm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLrJyEfbjWj0"
      },
      "source": [
        "# warmup exercise\n",
        "# questrom.datasets.diamonds\n",
        "# 1. write SQL to get the diamonds table from Big Query\n",
        "# 2. keep only numeric columns (pandas can be your friend here!)\n",
        "# 3. use kmeans to fit a 5 cluster solution\n",
        "# 4. generate the silohouette plot for the solution\n",
        "# 5. create a boxplot of the column carat by cluster label (one boxplot for each cluster)\n",
        "\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# print('Authenticated')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD_4dYv5bJ_9"
      },
      "source": [
        "##################################\n",
        "## PCA\n",
        "##################################\n",
        "\n",
        "# get the judges data from Big Query\n",
        "# we will use the same dataset as last week to start\n",
        "# questrom.datasets.judges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu6WIgtBmJll"
      },
      "source": [
        "# write a sql statement to get the judges dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhJiRoOEmVnW"
      },
      "source": [
        "# shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTjb90mLmZ7J"
      },
      "source": [
        "# columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCCZ7UR7mW5C"
      },
      "source": [
        "# cleanup\n",
        "\n",
        "# judges.index = judges.judge\n",
        "# del judges['judge']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Wxa8uqmegm"
      },
      "source": [
        "# first few rows to confirm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Z4mZNGmgcm"
      },
      "source": [
        "# lets review correlation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9K5GQFHoImj"
      },
      "source": [
        "## QUESTION:  2 minutes, review the judges correlation matrix\n",
        "##            if one way to reduce variables is to reduce via\n",
        "##            correlation, what would you select for variables?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TsL0fQnoa4G"
      },
      "source": [
        "# lets fit our first model\n",
        "# fit transform fits the PCA model AND applies \n",
        "# pca object has the fit data we will explore, pcs are the new features\n",
        "\n",
        "# pca = PCA()\n",
        "# pcs = pca.fit_transform(judges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUkOZ5vojmn"
      },
      "source": [
        "# what do we have?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfLn9Ntgo6h3"
      },
      "source": [
        "# shape confirmation (rows/features) are identical SHAPES\n",
        "\n",
        "# pcs.shape == judges.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxhguaUvo9pV"
      },
      "source": [
        "###################################\n",
        "# the core things we are looking for are the components (the new features)\n",
        "# the explained variance ratio for each, to help determine the cutoff\n",
        "# inverse_transform to put the data back into original  space\n",
        "###################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3gc_bLrpAqb"
      },
      "source": [
        "# first, lets get the explained variance\n",
        "# elbow plot\n",
        "\n",
        "# varexp = pca.explained_variance_ratio_\n",
        "# sns.lineplot(range(1, len(varexp)+1), varexp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaQ1RVEZtMoa"
      },
      "source": [
        "# cumulative variance\n",
        "\n",
        "# plt.title(\"Cumulative Explained Variance\")\n",
        "# plt.plot(range(1, len(varexp)+1), np.cumsum(varexp))\n",
        "# plt.axhline(.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq0aDdNJvhhQ"
      },
      "source": [
        "## EXERCISE:  Take 7 minutes:\n",
        "##            use the same diamonds dataset as the warmup exercise (from Big Query)\n",
        "##            do you have to do any data cleaning?\n",
        "##            EXCLUDE the price variable, we will use it later\n",
        "##            review the correlation matrix\n",
        "##            STANDARDIZE the data\n",
        "##            fit a PCA model \n",
        "##            generate plots to review how many PCs we might retain\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUHznOJR22Mm"
      },
      "source": [
        "##### Return to the judges dataset\n",
        "\n",
        "# quick function to construct the barplot easily\n",
        "def ev_plot(ev):\n",
        "  y = list(ev)\n",
        "  x = list(range(1,len(ev)+1))\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5YfMeXBxnpo"
      },
      "source": [
        "# going back to judges PCAs object pca\n",
        "# another approach for selection is to use explained variance > 1\n",
        "\n",
        "# x, y = ev_plot(pca.explained_variance_)\n",
        "# sns.barplot(x, y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwZVAn-kxnvs"
      },
      "source": [
        "## ok, lets step back\n",
        "## 1.  PCA we fit did not specify any settings, we will explore pre-config setup in a moment\n",
        "## 2.  Once we fit (.fit), the object has some useful information for us to explore\n",
        "## 3.  We extracted the components (new features) with .transform\n",
        "## 4.  We can use the summary info to inform our decisions as an analyst\n",
        "\n",
        "## But lets dive even deeper "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh-kIJtXxnzO"
      },
      "source": [
        "# original variable contributes to the PC construction\n",
        "# these are generally referred to as loadings\n",
        "# good resource: https://scentellegher.github.io/machine-learning/2020/01/27/pca-loadings-sklearn.html\n",
        "\n",
        "# component, feature\n",
        "\n",
        "\n",
        "# build column labels\n",
        "\n",
        "# COLS = [\"PC\" + str(i) for i in range(1, len(comps)+1)]\n",
        "# loadings = pd.DataFrame(comps.T, columns=COLS, index=judges.columns)\n",
        "# loadings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYxa2VJfxn2I"
      },
      "source": [
        "## so what do we have?\n",
        "\n",
        "# remember we talked about the linear combination?\n",
        "# these are the weights  contribution) used to construct the components from the original variables\n",
        "# TIP:  these are referred to as rotations in the output in R\n",
        "\n",
        "# What I tend to do is use this to try to explain how variables are contributing\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvBWvNQqYQdy"
      },
      "source": [
        "# help with hacking on matplotlib\n",
        "# https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUYPSNGxn8K"
      },
      "source": [
        "## 2 Minute thought exercise:\n",
        "##     Recall from earlier plots the variance explained and correlation\n",
        "##     Does this make sense?  Why or why not?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecP5qjdJxn-0"
      },
      "source": [
        "## Quick hands-on exercise:\n",
        "##     go back to your diamonds pca\n",
        "##     review the contribution of each variable on each PC\n",
        "##     what do you see?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceJJ6QgW0CYF"
      },
      "source": [
        "# moving forward, application 1 is to use these new feautres instead of the oringal features\n",
        "\n",
        "# quick refresher, review pcs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3PlAdZF0CU_"
      },
      "source": [
        "# shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kdCjKa40CSa"
      },
      "source": [
        "# slice all \"rows\", and the first 2 components\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQqUMg5x0CMg"
      },
      "source": [
        "# first few rows and columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRENIS-S0CD0"
      },
      "source": [
        "# make a new dataframe\n",
        "# remember, these are what we might use if our task was to learn a model\n",
        "\n",
        "# j = pd.DataFrame(comps, columns=['pc1', 'pc2'], index=judges.index)\n",
        "# j.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M59fSbtA0BzO"
      },
      "source": [
        "## notice that I am NOT putting these back onto the original\n",
        "## you can of course, but the point is that these are now our new features for any other downstream tasks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdOwTLFq3i6W"
      },
      "source": [
        "# viz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdIxl6sS30QV"
      },
      "source": [
        "## Thought exercise:\n",
        "##       what do you see in the plot above?  Anything\n",
        "##       what might you want to do now?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er4Y1k6f3nWz"
      },
      "source": [
        "## Quick exercise:\n",
        "##       using the two pcs from judges\n",
        "##       fit keans = 2\n",
        "##       plot the same scatter plot from above, but color the points by cluster assignment\n",
        "##       What do you think about the cluster assignments?\n",
        "##       What might you do to improve?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erRKGJjv6bXg"
      },
      "source": [
        "#############################\n",
        "## some options when fitting PCA Model\n",
        "## we can pre-specify the number of components OR the % of var we want to explain!\n",
        "## lets use diamonds\n",
        "\n",
        "# lets rebuild to practice and show some quick 1-liners\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdzQ3mef6_oB"
      },
      "source": [
        "# arbitrary, but lets keep just 2 components\n",
        "# using svd_solver full to keep solutions consistent\n",
        "# scikit tries to be smart with auto, but I want to the same solver for examples\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_rJ_2DD_a5Q"
      },
      "source": [
        "# variance explained in just 2?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLQSGsKh71yb"
      },
      "source": [
        "# I will violate my rule and put back onto the original just for exploration\n",
        "\n",
        "# dia[['pc2_1', 'pc2_2']] = dia_pc2\n",
        "# dia.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNjQbr1_73iV"
      },
      "source": [
        "# plot it up\n",
        "# https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
        "\n",
        "# dia['cut2'] = dia.cut.astype('category').cat.codes\n",
        "# plt.scatter(x=dia.pc2_1, y=dia.pc2_2, c=dia.cut2, cmap=cm.Paired, alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is816RAE9YPM"
      },
      "source": [
        "# lets fit another PCA, but keep 90% variance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IVPT2OX9zhp"
      },
      "source": [
        "# Violate again, see results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W43q9Uc1AH3B"
      },
      "source": [
        "# now lets look at everything on the original dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzVs-kplAKOq"
      },
      "source": [
        "##################################################################\n",
        "##\n",
        "##   Compression\n",
        "## \n",
        "## once we have fit our model, we have seen that transform generaâ€ es the newly constructed features\n",
        "## but we can also project back into the original feature space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Zbryw5BVP7"
      },
      "source": [
        "# quick refresher, what was the shape that we used?\n",
        "# diamonds pcs we just fit, 2 and 90\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UlKFfC5OsEI"
      },
      "source": [
        "# lets use 90% (3 components)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInEbt03PG0l"
      },
      "source": [
        "# put back into a dataframe\n",
        "# remember, I scaled dia_n -> dia_s\n",
        "\n",
        "# comp3df = pd.DataFrame(comp3, columns=dia_n.columns)\n",
        "# comp3df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiWA5-DXPdVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKBS2wP0Ppaz"
      },
      "source": [
        "# remember the MNIST dataset?\n",
        "# I showed the aspect of compression there, a simple link\n",
        "# the digit is reconstructed after fitting N components and .inverse_tranform\n",
        "# depends on our use case of course, but highlights that \"losing\" information may not hurt your applications\n",
        "# depending on the dataset, our problem, and our method, we can potentially remove noise and only retain signal!\n",
        "\n",
        "# https://snipboard.io/qrohR7.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lo8IraJQ_rx"
      },
      "source": [
        "##################################################################\n",
        "##\n",
        "##   Next steps\n",
        "## \n",
        "## - Diamonds data challenge in breakout rooms\n",
        "## - lets start to see how we can combine UML and SML!\n",
        "##\n",
        "## - OBJECTIVE:  As a group, fit a regression model to the price column\n",
        "## -             What is your R2? can you beat your best score?\n",
        "## \n",
        "##\n",
        "## 1. refit PCA to the diamonds dataset.\n",
        "## 2. how many components would you select\n",
        "## 3. remember!  do not include price in your columns when generating the components, we are predicint that!\n",
        "## 4. Iterate!  try different models, assumptions\n",
        "##\n",
        "## NOTE:  we haven't covered regression in scikit learn, but its the same flow!\n",
        "## Some help:  \n",
        "##   \n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import r2_score \n",
        "##################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPJqz_YXkwIp"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}